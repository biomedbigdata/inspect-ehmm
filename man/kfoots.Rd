% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kfoots.R
\name{kfoots}
\alias{kfoots}
\title{Models for multivariate count data
Fit mixture model or a hidden markov model}
\usage{
kfoots(counts, k, framework = c("HMM", "MM"), mix_coeff = NULL,
  trans = NULL, initP = NULL, tol = 1e-04, maxiter = 200,
  nthreads = 1, nbtype = c("dep", "indep", "pois", "lognormal"),
  init = c("pca", "counts", "rnd"), init.nlev = 20, verbose = TRUE,
  seqlens = ncol(counts), split4speed = FALSE,
  endstate = as.numeric(NULL), trainMode = "baum-welch",
  fix_emisP = FALSE, notrain = FALSE, labels)
}
\arguments{
\item{counts}{matrix of non-negative integers. Columns represent datapoints and rows
dimensions}

\item{k}{either the desired number of cluster, or a specific initial
value for the models (mixture components or emission probabilities).
See the item \code{models} in the return values to see how the
model parameters should be formatted}

\item{framework}{Switches between a mixture model and a hidden markov model.
The default is a hidden markov model, where the order of the datapoints
matters.}

\item{mix_coeff}{In the \code{MM} mode, initial value for the mixture 
coefficients. In the \code{HMM} mode it will be ignored.}

\item{trans}{In the \code{HMM} mode, initial value for the transition
probabilities as a square matrix. The rows are the 'state from' and 
the columns are the 'state to', so each rows must sum up to 1. 
In the \code{HMM} mode it will be ignored.}

\item{initP}{In the \code{HMM} mode, initial probabilities for each 
sequence of observation. They must be formatted as a matrix where 
each row is a state and each column is a sequence.}

\item{tol}{Tolerance value used to determine convergence of the EM
algorithm. The algorithm will converge when the absolute difference
in the log-likelihood between two iterations will fall below this value.}

\item{maxiter}{maximum number of iterations in the EM algorithm. Use 0 if 
you don't want to do any training iteration.}

\item{nthreads}{number of threads used. The backward-forward step in the HMM learning
cannot use more threads than the number of sequences.}

\item{nbtype}{type of training for the negative binomial. Accepted types are:
\code{indep}, \code{dep}, \code{pois}, \code{lognormal}. The first type corresponds to standard
maximum likelihood estimates for each parameter of each model, the second one
forces the \code{r} dispersion parameters of the negative multinomials to be the same
for all models, the third one forces \code{r} to be infinity, that is, every model
will be a Poisson distribution. The fourth corresponds to a lognormal distribution.}

\item{init}{Initialization scheme for the models (mixture components or emission
probabilities). The value \code{rnd} results in parameters being chosen randomly,
the values \code{counts, pca} use an initialization algorithm that starts from
\code{init.nlev*nrow(counts)} clusters and reduces them to \code{k} using
hierachical clustering.}

\item{init.nlev}{Tuning parameter for the initialization schemes \code{counts, pca}.}

\item{verbose}{print some output during execution}

\item{seqlens}{Length of each sequence of observations. The number of columns
of the count matrix should equal \code{sum(seqlens)}.}

\item{split4speed}{Add artificial breaks to speed-up the forward-backward
algorithm. If \code{framework=="HMM"} and if multiple threads are used,
the count matrix, which is already split according to \code{seqlens}, is
split even further so that each thread can be assigned an equal amount
of observations in the forward-backward algorithm. These artificial breaks
usually have a small impact in the final parameters, and they improve the
scalability with the number of cores, especially when the number of sequences
is small compared to the number of cores. The artificial breaks are 
removed after the training phase for computing the final state assignments.}

\item{endstate}{Vector of state numbers allowed to occur at the last position.
If an endstate is given, the trainMode is set to viterbi.}

\item{trainMode}{Choose between viterbi and baum-welch training mode (default: baum-welch)}

\item{fix_emisP}{set this flag if training should only affect transition probabilities (emission probabilities will be fixed)}

\item{labels}{vector with state labels.}
}
\value{
a list with, among other, the following parameters:
    \item{models}{a list containing the parameters of each model
    (mixture components or emission probabilities). Each element of 
    the list describes a negative multinomial distribution.
This is specified in another list with items \code{mu}, \code{r} and \code{ps}. \code{mu} and
        \code{r} correspond to parameters \code{mu} and \code{size} in the R-function \code{\link{dnbinom}}.
        Ps specifies the parameters of the multinomial and they sum up to 1.}
    \item{loglik}{the log-likelihood of the whole dataset.}
   \item{posteriors}{A matrix of size \code{length(models)*ncol(counts)} containing the posterior
           probability that a given datapoint is generated by the given mixture component}
    \item{states}{An integer vector of length \code{ncol(counts)} saying
        which model each column is associated to (using the posterior decoding
        algorithm).}
   \item{converged}{\code{TRUE} if the algorithm converged in the given number of iterations, \code{FALSE} otherwise}
   \item{llhistory}{time series containing the log-likelihood of the
       whole dataset across iterations}
    \item{viterbi}{In HMM mode, the viterbi path an its likelihood as a list.}
}
\description{
Models for multivariate count data
Fit mixture model or a hidden markov model
}
